{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import quote_plus\n",
    "from logger import get_logger\n",
    "logger = get_logger('main')\n",
    "\n",
    "from utils.selenium_utils import ScraperConfig\n",
    "from utils.common import scroll_and_wait, load_and_scroll, sleeper, save_as, soup_maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extractor(soup: BeautifulSoup, base_url: str) -> List[dict]:\n",
    "    job_list = soup.find('div', id='mosaic-provider-jobcards')\n",
    "    if not job_list:\n",
    "        return []\n",
    "\n",
    "    jobs = job_list.find('ul').find_all('li')[1:]  # Skip the first empty card or header\n",
    "    result = []\n",
    "\n",
    "    for job in jobs:\n",
    "        try:\n",
    "            title_tag = job.find('h2')\n",
    "            title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "            company_tag = job.select_one('div.company_location span[data-testid=\"company-name\"]')\n",
    "            company_name = company_tag.text.strip() if company_tag else None\n",
    "\n",
    "            job_type_tag = job.select_one('div[data-testid=\"text-location\"] span')\n",
    "            job_type = job_type_tag.text.strip() if job_type_tag else None\n",
    "\n",
    "            extra_items = job.select('div[data-testid=\"jobsnippet_footer\"] ul li')\n",
    "            extra_data = [item.text.strip() for item in extra_items] if extra_items else []\n",
    "\n",
    "            link_tag = title_tag.find('a') if title_tag else None\n",
    "            link = f\"{base_url}{link_tag['href']}\" if link_tag and 'href' in link_tag.attrs else None\n",
    "\n",
    "            if title and company_name and link:  # Minimum data check\n",
    "                job_data = {\n",
    "                    'title': title,\n",
    "                    'company_name': company_name,\n",
    "                    'job_type': job_type,\n",
    "                    'extra_data': extra_data,\n",
    "                    'link': link,\n",
    "                }\n",
    "                result.append(job_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing job: {e}\")\n",
    "            continue\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndeedScraper:\n",
    "    def __init__(self, config: ScraperConfig):\n",
    "        self.config = config\n",
    "        self.driver = config.driver\n",
    "        self.url = \"https://www.indeed.com\"\n",
    "        logger.info(\"🚀 IndeedScraper initialized\")\n",
    "\n",
    "    def get_search_url(self, keyword: str) -> str:\n",
    "        encoded = quote_plus(keyword)\n",
    "        search_url = f\"{self.url}/jobs?q={encoded}\"\n",
    "        logger.debug(f\"🔗 Generated search URL: {search_url}\")\n",
    "        return search_url\n",
    "\n",
    "    def scrape_search_results(self, keyword: str, wait_time=3) -> str:\n",
    "        url = self.get_search_url(keyword)\n",
    "        logger.info(f\"🌐 Navigating to search page: {url}\")\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            scroll_and_wait(self.driver)\n",
    "            time.sleep(wait_time)\n",
    "            logger.info(\"✅ Page loaded and ready for scraping\")\n",
    "            return self.driver.page_source\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error loading page for keyword '{keyword}': {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def pagination(self, soup):\n",
    "        logger.debug(\"📄 Checking for pagination...\")\n",
    "\n",
    "        try:\n",
    "            # Find the \"Next Page\" button\n",
    "            next_page = soup.find('a', attrs={\"data-testid\": \"pagination-page-next\", \"aria-label\": \"Next Page\"})\n",
    "\n",
    "            if not next_page or not next_page.get(\"href\"):\n",
    "                logger.warning(\"⚠️ No next page link found\")\n",
    "                return None\n",
    "\n",
    "            # Combine base URL and relative href\n",
    "            next_url = urljoin(self.url, next_page[\"href\"])\n",
    "\n",
    "            logger.info(f\"➡️ Found next page URL: {next_url}\")\n",
    "            return next_url\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Pagination error: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_all_pages(self, keyword: str, max_pages=5):\n",
    "        results = []\n",
    "        response = self.scrape_search_results(keyword)\n",
    "        soup = soup_maker(response)\n",
    "        page = 0\n",
    "         \n",
    "        while soup and page < max_pages:\n",
    "            data = extractor(soup, self.url)\n",
    "            results += data\n",
    "\n",
    "            logger.info(f'📄 Page {page + 1} scraped.')\n",
    "\n",
    "            next_page = self.pagination(soup)\n",
    "            if not next_page:\n",
    "                break\n",
    "            \n",
    "            self.driver.get(next_page)\n",
    "            time.sleep(random.uniform(2, 4))  # can swap with sleeper()\n",
    "            soup = soup_maker(self.driver.page_source)\n",
    "            page += 1\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "    def quit(self):\n",
    "        logger.info(\"🛑 Quitting WebDriver\")\n",
    "        self.driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# config = ScraperConfig(use_scrapeops=True,use_seleniumwire=True)\n",
    "config = ScraperConfig(use_uc=True)\n",
    "indeed = IndeedScraper(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "results =indeed.scrape_all_pages('software delveloper',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as(results,'x.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
